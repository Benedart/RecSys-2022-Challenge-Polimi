{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems 2021/22\n",
    "\n",
    "### Practice - Implicit Alternating Least Squares\n",
    "\n",
    "See:\n",
    "Y. Hu, Y. Koren and C. Volinsky, Collaborative filtering for implicit feedback datasets, ICDM 2008.\n",
    "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.167.5120&rep=rep1&type=pdf\n",
    "\n",
    "R. Pan et al., One-class collaborative filtering, ICDM 2008.\n",
    "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.306.4684&rep=rep1&type=pdf\n",
    "\n",
    "Factorization model for binary feedback.\n",
    "First, splits the feedback matrix R as the element-wise a Preference matrix P and a Confidence matrix C.\n",
    "Then computes the decomposition of them into the dot product of two matrices X and Y of latent factors.\n",
    "X represent the user latent factors, Y the item latent factors.\n",
    "\n",
    "The model is learned by solving the following regularized Least-squares objective function with Stochastic Gradient Descent\n",
    "    \n",
    "$$\\frac{1}{2}\\sum_{i,j}{c_{ij}\\left(p_{ij}-x_i^T y_j\\right) + \\lambda\\left(\\sum_{i}{||x_i||^2} + \\sum_{j}{||y_j||^2}\\right)}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movielens10M: Verifying data consistency...\n",
      "Movielens10M: Verifying data consistency... Passed!\n",
      "DataReader: current dataset is: <class 'Data_manager.Dataset.Dataset'>\n",
      "\tNumber of items: 10681\n",
      "\tNumber of users: 69878\n",
      "\tNumber of interactions in URM_all: 10000054\n",
      "\tValue range in URM_all: 0.50-5.00\n",
      "\tInteraction density: 1.34E-02\n",
      "\tInteractions per user:\n",
      "\t\t Min: 2.00E+01\n",
      "\t\t Avg: 1.43E+02\n",
      "\t\t Max: 7.36E+03\n",
      "\tInteractions per item:\n",
      "\t\t Min: 0.00E+00\n",
      "\t\t Avg: 9.36E+02\n",
      "\t\t Max: 3.49E+04\n",
      "\tGini Index: 0.57\n",
      "\n",
      "\tICM name: ICM_genres, Value range: 1.00 / 1.00, Num features: 20, feature occurrences: 21564, density 1.01E-01\n",
      "\tICM name: ICM_all, Value range: 1.00 / 69.00, Num features: 10126, feature occurrences: 128384, density 1.19E-03\n",
      "\tICM name: ICM_year, Value range: 6.00E+00 / 2.01E+03, Num features: 1, feature occurrences: 10681, density 1.00E+00\n",
      "\tICM name: ICM_tags, Value range: 1.00 / 69.00, Num features: 10106, feature occurrences: 106820, density 9.90E-04\n",
      "\n",
      "\n",
      "Warning: 66 (0.09 %) of 69878 users have no sampled items\n"
     ]
    }
   ],
   "source": [
    "from Data_manager.split_functions.split_train_validation_random_holdout import split_train_in_two_percentage_global_sample\n",
    "from Data_manager.Movielens.Movielens10MReader import Movielens10MReader\n",
    "\n",
    "data_reader = Movielens10MReader()\n",
    "data_loaded = data_reader.load_data()\n",
    "\n",
    "URM_all = data_loaded.get_URM_all()\n",
    "\n",
    "URM_train, URM_test = split_train_in_two_percentage_global_sample(URM_all, train_percentage = 0.80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<69878x10681 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 8000043 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URM_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do we need for IALS?\n",
    "\n",
    "* User factor and Item factor matrices\n",
    "* Confidence function\n",
    "* Update rule for items\n",
    "* Update rule for users\n",
    "* Training loop and some patience\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users, n_items = URM_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: We create the dense latent factor matrices\n",
    "### In a MF model you have two matrices, one with a row per user and the other with a column per item. The other dimension, columns for the first one and rows for the second one is called latent factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_factors = 10\n",
    "\n",
    "user_factors = np.random.random((n_users, num_factors))\n",
    "item_factors = np.random.random((n_items, num_factors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.68170924, 0.45510017, 0.38803449, ..., 0.68549916, 0.46539247,\n",
       "        0.06620504],\n",
       "       [0.06305243, 0.95881947, 0.06663733, ..., 0.04360239, 0.07490854,\n",
       "        0.22469506],\n",
       "       [0.04097471, 0.39420552, 0.5667942 , ..., 0.79911491, 0.08627878,\n",
       "        0.5937455 ],\n",
       "       ...,\n",
       "       [0.11816292, 0.4737209 , 0.7678413 , ..., 0.60987589, 0.29662157,\n",
       "        0.18942834],\n",
       "       [0.66924699, 0.03590675, 0.07888202, ..., 0.46117881, 0.72213628,\n",
       "        0.23674881],\n",
       "       [0.20848604, 0.04554099, 0.28709882, ..., 0.25089374, 0.89577473,\n",
       "        0.03565192]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.15069107, 0.80681573, 0.30778298, ..., 0.14677423, 0.23341278,\n",
       "        0.37704531],\n",
       "       [0.92211302, 0.44460682, 0.7913981 , ..., 0.78896563, 0.1446843 ,\n",
       "        0.92989713],\n",
       "       [0.23538478, 0.99340528, 0.81193351, ..., 0.845437  , 0.74659047,\n",
       "        0.17975963],\n",
       "       ...,\n",
       "       [0.12806035, 0.5234029 , 0.19163775, ..., 0.40652814, 0.60689866,\n",
       "        0.15534618],\n",
       "       [0.30970641, 0.62823476, 0.67904598, ..., 0.46228659, 0.09578545,\n",
       "        0.60984898],\n",
       "       [0.92799839, 0.37126583, 0.74574018, ..., 0.98925788, 0.67638146,\n",
       "        0.51500969]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: We define a function to transform the interaction data in a \"confidence\" value. \n",
    "* If you have explicit data, the higher it is the higher the confidence (logarithmic, linear?)\n",
    "* Other options include scaling the data lowering it if the item or use has very few interactions (lower support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_confidence_function(URM_train, alpha):\n",
    "    \n",
    "    URM_train.data = 1.0 + alpha*URM_train.data\n",
    "    \n",
    "    return URM_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = 0.5\n",
    "C_URM_train = linear_confidence_function(URM_train, alpha)\n",
    "\n",
    "C_URM_train.data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept of confidence can be defined in different ways, for example in terms of the number of interactions an item or a user has, the more they have the more support your model will have for the respective latent factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def popularity_confidence(URM_train):\n",
    "    \n",
    "    item_popularity = np.ediff1d(URM_train.tocsc().indptr)\n",
    "    item_confidence = np.zeros(len(item_popularity))\n",
    "    item_confidence[item_popularity!=0] = np.log(item_popularity[item_popularity!=0])\n",
    "    \n",
    "    C_URM_train = URM_train.copy()\n",
    "    C_URM_train = C_URM_train.tocsc()\n",
    "    \n",
    "    for item_id in range(C_URM_train.shape[1]):\n",
    "        start_pos = C_URM_train.indices[item_id]\n",
    "        end_pos = C_URM_train.indices[item_id+1]\n",
    "        \n",
    "        C_URM_train.data[start_pos:end_pos] = item_confidence[item_id]\n",
    "    \n",
    "    C_URM_train = C_URM_train.tocsr()\n",
    "    \n",
    "    return C_URM_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.57471098, 4.41884061, 5.70711026, 1.79175947, 7.47986413,\n",
       "       3.5       , 3.5       , 3.5       , 3.5       , 3.5       ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C_URM_train = popularity_confidence(URM_train)\n",
    "\n",
    "C_URM_train.data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define the update rules for the user factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Update latent factors for a single user or item.\n",
    "\n",
    "Y = |n_interactions|x|n_factors|\n",
    "\n",
    "YtY =   |n_factors|x|n_factors|\n",
    "\n",
    "\n",
    "\n",
    "Latent factors ony of item/users for which an interaction exists in the interaction profile\n",
    "Y_interactions = Y[interaction_profile, :]\n",
    "\n",
    "Following the notation of the original paper we report the update rule for the Item factors (User factors are identical):\n",
    "* __Y__ are the item factors |n_items|x|n_factors|\n",
    "* __Cu__ is a diagonal matrix |n_interactions|x|n_interactions| with the user confidence for the observed items\n",
    "* __p(u)__ is a boolean vectors indexing only observed items. Here it will disappear as we already extract only the observed latent factors however, it will have an impact in the dimensions of the matrix, since it transforms Cu from a diagonal matrix to a row vector of 1 row and |n_interactions| columns\n",
    "\n",
    "$$(Yt*Cu*Y + reg*I)^-1 * Yt*Cu*profile$$ which can be decomposed as $$(YtY + Yt*(Cu-I)*Y + reg*I)^-1 * Yt*Cu*p(u)$$ \n",
    "\n",
    "* __A__ = (|n_interactions|x|n_factors|) dot (|n_interactions|x|n_interactions| ) dot (|n_interactions|x|n_factors| )\n",
    "  = |n_factors|x|n_factors|\n",
    "  \n",
    "We use an equivalent formulation (v * k.T).T which is much faster\n",
    "* __A__ = Y_interactions.T.dot(((interaction_confidence - 1) * Y_interactions.T).T)\n",
    "* __B__ = YtY + A + self.regularization_diagonal\n",
    "* __new factors__ = np.dot(np.linalg.inv(B), Y_interactions.T.dot(interaction_confidence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _update_row(interaction_profile, interaction_confidence, Y, YtY, regularization_diagonal):\n",
    "\n",
    "    Y_interactions = Y[interaction_profile, :]\n",
    "    \n",
    "    A = Y_interactions.T.dot(((interaction_confidence - 1) * Y_interactions.T).T)\n",
    "\n",
    "    B = YtY + A + regularization_diagonal\n",
    "\n",
    "    return np.dot(np.linalg.inv(B), Y_interactions.T.dot(interaction_confidence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0001, 0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    ,\n",
       "        0.    , 0.    ],\n",
       "       [0.    , 0.0001, 0.    , 0.    , 0.    , 0.    , 0.    , 0.    ,\n",
       "        0.    , 0.    ],\n",
       "       [0.    , 0.    , 0.0001, 0.    , 0.    , 0.    , 0.    , 0.    ,\n",
       "        0.    , 0.    ],\n",
       "       [0.    , 0.    , 0.    , 0.0001, 0.    , 0.    , 0.    , 0.    ,\n",
       "        0.    , 0.    ],\n",
       "       [0.    , 0.    , 0.    , 0.    , 0.0001, 0.    , 0.    , 0.    ,\n",
       "        0.    , 0.    ],\n",
       "       [0.    , 0.    , 0.    , 0.    , 0.    , 0.0001, 0.    , 0.    ,\n",
       "        0.    , 0.    ],\n",
       "       [0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.0001, 0.    ,\n",
       "        0.    , 0.    ],\n",
       "       [0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.0001,\n",
       "        0.    , 0.    ],\n",
       "       [0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    ,\n",
       "        0.0001, 0.    ],\n",
       "       [0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    ,\n",
       "        0.    , 0.0001]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regularization_coefficient = 1e-4\n",
    "\n",
    "regularization_diagonal = np.diag(regularization_coefficient * np.ones(num_factors))\n",
    "regularization_diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VV = n_factors x n_factors\n",
    "VV = item_factors.T.dot(item_factors)\n",
    "VV.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 154"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_URM_train = linear_confidence_function(URM_train, alpha)\n",
    "\n",
    "start_pos = C_URM_train.indptr[user_id]\n",
    "end_pos = C_URM_train.indptr[user_id + 1]\n",
    "\n",
    "user_profile = C_URM_train.indices[start_pos:end_pos]\n",
    "user_confidence = C_URM_train.data[start_pos:end_pos]\n",
    "\n",
    "user_factors[user_id, :] = _update_row(user_profile, user_confidence, item_factors, VV, regularization_diagonal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Apply updates on the user item factors as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# UU = n_factors x n_factors\n",
    "UU = user_factors.T.dot(user_factors)\n",
    "UU.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_id = 154"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_URM_train_csc = C_URM_train.tocsc()\n",
    "\n",
    "start_pos = C_URM_train_csc.indptr[item_id]\n",
    "end_pos = C_URM_train_csc.indptr[item_id + 1]\n",
    "\n",
    "item_profile = C_URM_train_csc.indices[start_pos:end_pos]\n",
    "item_confidence = C_URM_train_csc.data[start_pos:end_pos]\n",
    "\n",
    "item_factors[item_id, :] = _update_row(item_profile, item_confidence, user_factors, UU, regularization_diagonal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's put all together in a training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 69878 in 56.18 seconds. Users per second 1243.74\n",
      "Iteration 10681 in 75.47 seconds. Items per second 141.51\n",
      "Epoch 1 complete in in 75.47 seconds\n",
      "Iteration 69878 in 62.83 seconds. Users per second 1112.08\n",
      "Iteration 10681 in 78.29 seconds. Items per second 136.41\n",
      "Epoch 2 complete in in 78.29 seconds\n",
      "Iteration 69878 in 50.60 seconds. Users per second 1380.85\n",
      "Iteration 10681 in 68.83 seconds. Items per second 155.17\n",
      "Epoch 3 complete in in 68.83 seconds\n",
      "Iteration 69878 in 51.41 seconds. Users per second 1359.21\n",
      "Iteration 10681 in 61.39 seconds. Items per second 173.97\n",
      "Epoch 4 complete in in 61.39 seconds\n",
      "Iteration 69878 in 24.50 seconds. Users per second 2852.00\n",
      "Iteration 10681 in 30.12 seconds. Items per second 354.53\n",
      "Epoch 5 complete in in 30.12 seconds\n",
      "Iteration 69878 in 25.73 seconds. Users per second 2715.40\n",
      "Iteration 10681 in 33.49 seconds. Items per second 318.94\n",
      "Epoch 6 complete in in 33.49 seconds\n",
      "Iteration 69878 in 15.25 seconds. Users per second 4580.85\n",
      "Iteration 10681 in 21.50 seconds. Items per second 496.79\n",
      "Epoch 7 complete in in 21.50 seconds\n",
      "Iteration 69878 in 19.91 seconds. Users per second 3510.02\n",
      "Iteration 10681 in 28.41 seconds. Items per second 375.90\n",
      "Epoch 8 complete in in 28.41 seconds\n",
      "Iteration 69878 in 30.34 seconds. Users per second 2302.92\n",
      "Iteration 10681 in 45.18 seconds. Items per second 236.41\n",
      "Epoch 9 complete in in 45.18 seconds\n",
      "Iteration 69878 in 50.39 seconds. Users per second 1386.86\n",
      "Iteration 10681 in 66.80 seconds. Items per second 159.88\n",
      "Epoch 10 complete in in 66.80 seconds\n"
     ]
    }
   ],
   "source": [
    "C_URM_train_csc = C_URM_train.tocsc()\n",
    "\n",
    "num_factors = 10\n",
    "\n",
    "user_factors = np.random.random((n_users, num_factors))\n",
    "item_factors = np.random.random((n_items, num_factors))\n",
    "\n",
    "\n",
    "for n_epoch in range(10):\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    VV = item_factors.T.dot(item_factors)\n",
    "        \n",
    "    for user_id in range(C_URM_train.shape[0]):\n",
    "\n",
    "        start_pos = C_URM_train.indptr[user_id]\n",
    "        end_pos = C_URM_train.indptr[user_id + 1]\n",
    "\n",
    "        user_profile = C_URM_train.indices[start_pos:end_pos]\n",
    "        user_confidence = C_URM_train.data[start_pos:end_pos]\n",
    "        \n",
    "        user_factors[user_id, :] = _update_row(user_profile, user_confidence, item_factors, VV, regularization_diagonal)   \n",
    "\n",
    "        # Print some stats\n",
    "        if (user_id +1)% 100000 == 0 or user_id == C_URM_train.shape[0]-1:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            samples_per_second = user_id/elapsed_time\n",
    "            print(\"Iteration {} in {:.2f} seconds. Users per second {:.2f}\".format(user_id+1, elapsed_time, samples_per_second))\n",
    "    \n",
    "    UU = user_factors.T.dot(user_factors)\n",
    "\n",
    "    for item_id in range(C_URM_train.shape[1]):\n",
    "\n",
    "        start_pos = C_URM_train_csc.indptr[item_id]\n",
    "        end_pos = C_URM_train_csc.indptr[item_id + 1]\n",
    "\n",
    "        item_profile = C_URM_train_csc.indices[start_pos:end_pos]\n",
    "        item_confidence = C_URM_train_csc.data[start_pos:end_pos]\n",
    "\n",
    "        item_factors[item_id, :] = _update_row(item_profile, item_confidence, user_factors, UU, regularization_diagonal)    \n",
    "\n",
    "        # Print some stats\n",
    "        if (item_id +1)% 100000 == 0 or item_id == C_URM_train.shape[1]-1:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            samples_per_second = item_id/elapsed_time\n",
    "            print(\"Iteration {} in {:.2f} seconds. Items per second {:.2f}\".format(item_id+1, elapsed_time, samples_per_second))\n",
    "\n",
    "    total_epoch_time = time.time() - start_time  \n",
    "    print(\"Epoch {} complete in in {:.2f} seconds\".format(n_epoch+1, total_epoch_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How long do we train such a model?\n",
    "\n",
    "* An epoch: a complete loop over all the train data\n",
    "* Usually you train for multiple epochs. Depending on the algorithm and data 10s or 100s of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated time with the previous training speed is 668.00 seconds, or 11.13 minutes\n"
     ]
    }
   ],
   "source": [
    "estimated_seconds = total_epoch_time*10\n",
    "print(\"Estimated time with the previous training speed is {:.2f} seconds, or {:.2f} minutes\".format(estimated_seconds, estimated_seconds/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lastly: Computing a prediction for any given user or item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 17025\n",
    "item_id = 468"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0313089655029795"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_rating = np.dot(user_factors[user_id,:], item_factors[item_id,:])\n",
    "predicted_rating"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
